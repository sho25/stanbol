begin_unit|revision:0.9.5;language:Java;cregit-version:0.0.1
begin_package
package|package
name|eu
operator|.
name|iksproject
operator|.
name|fise
operator|.
name|engines
operator|.
name|dulcifier
package|;
end_package

begin_import
import|import
name|eu
operator|.
name|iksproject
operator|.
name|kres
operator|.
name|api
operator|.
name|rules
operator|.
name|util
operator|.
name|KReSRuleList
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|FileNotFoundException
import|;
end_import

begin_import
import|import
name|java
operator|.
name|io
operator|.
name|InputStream
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Collections
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|HashSet
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Iterator
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Map
import|;
end_import

begin_import
import|import
name|java
operator|.
name|util
operator|.
name|Set
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|clerezza
operator|.
name|rdf
operator|.
name|core
operator|.
name|MGraph
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|clerezza
operator|.
name|rdf
operator|.
name|core
operator|.
name|Resource
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|clerezza
operator|.
name|rdf
operator|.
name|core
operator|.
name|Triple
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|clerezza
operator|.
name|rdf
operator|.
name|core
operator|.
name|UriRef
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|felix
operator|.
name|scr
operator|.
name|annotations
operator|.
name|Component
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|felix
operator|.
name|scr
operator|.
name|annotations
operator|.
name|Property
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|felix
operator|.
name|scr
operator|.
name|annotations
operator|.
name|Reference
import|;
end_import

begin_import
import|import
name|org
operator|.
name|apache
operator|.
name|felix
operator|.
name|scr
operator|.
name|annotations
operator|.
name|Service
import|;
end_import

begin_import
import|import
name|org
operator|.
name|osgi
operator|.
name|service
operator|.
name|component
operator|.
name|ComponentContext
import|;
end_import

begin_import
import|import
name|org
operator|.
name|semanticweb
operator|.
name|owlapi
operator|.
name|apibinding
operator|.
name|OWLManager
import|;
end_import

begin_import
import|import
name|org
operator|.
name|semanticweb
operator|.
name|owlapi
operator|.
name|io
operator|.
name|RDFXMLOntologyFormat
import|;
end_import

begin_import
import|import
name|org
operator|.
name|semanticweb
operator|.
name|owlapi
operator|.
name|model
operator|.
name|IRI
import|;
end_import

begin_import
import|import
name|org
operator|.
name|semanticweb
operator|.
name|owlapi
operator|.
name|model
operator|.
name|OWLOntology
import|;
end_import

begin_import
import|import
name|org
operator|.
name|semanticweb
operator|.
name|owlapi
operator|.
name|model
operator|.
name|OWLOntologyCreationException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|semanticweb
operator|.
name|owlapi
operator|.
name|model
operator|.
name|OWLOntologyManager
import|;
end_import

begin_import
import|import
name|org
operator|.
name|semanticweb
operator|.
name|owlapi
operator|.
name|model
operator|.
name|OWLOntologySetProvider
import|;
end_import

begin_import
import|import
name|org
operator|.
name|semanticweb
operator|.
name|owlapi
operator|.
name|model
operator|.
name|OWLOntologyStorageException
import|;
end_import

begin_import
import|import
name|org
operator|.
name|semanticweb
operator|.
name|owlapi
operator|.
name|util
operator|.
name|OWLOntologyMerger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|Logger
import|;
end_import

begin_import
import|import
name|org
operator|.
name|slf4j
operator|.
name|LoggerFactory
import|;
end_import

begin_import
import|import
name|eu
operator|.
name|iksproject
operator|.
name|fise
operator|.
name|dereferencing
operator|.
name|IDereferencer
import|;
end_import

begin_import
import|import
name|eu
operator|.
name|iksproject
operator|.
name|fise
operator|.
name|servicesapi
operator|.
name|ContentItem
import|;
end_import

begin_import
import|import
name|eu
operator|.
name|iksproject
operator|.
name|fise
operator|.
name|servicesapi
operator|.
name|EngineException
import|;
end_import

begin_import
import|import
name|eu
operator|.
name|iksproject
operator|.
name|fise
operator|.
name|servicesapi
operator|.
name|EnhancementEngine
import|;
end_import

begin_import
import|import
name|eu
operator|.
name|iksproject
operator|.
name|fise
operator|.
name|servicesapi
operator|.
name|ServiceProperties
import|;
end_import

begin_import
import|import
name|eu
operator|.
name|iksproject
operator|.
name|kres
operator|.
name|api
operator|.
name|manager
operator|.
name|DuplicateIDException
import|;
end_import

begin_import
import|import
name|eu
operator|.
name|iksproject
operator|.
name|kres
operator|.
name|api
operator|.
name|manager
operator|.
name|KReSONManager
import|;
end_import

begin_import
import|import
name|eu
operator|.
name|iksproject
operator|.
name|kres
operator|.
name|api
operator|.
name|manager
operator|.
name|ontology
operator|.
name|OntologyInputSource
import|;
end_import

begin_import
import|import
name|eu
operator|.
name|iksproject
operator|.
name|kres
operator|.
name|api
operator|.
name|manager
operator|.
name|ontology
operator|.
name|OntologyScope
import|;
end_import

begin_import
import|import
name|eu
operator|.
name|iksproject
operator|.
name|kres
operator|.
name|api
operator|.
name|manager
operator|.
name|ontology
operator|.
name|OntologyScopeFactory
import|;
end_import

begin_import
import|import
name|eu
operator|.
name|iksproject
operator|.
name|kres
operator|.
name|api
operator|.
name|manager
operator|.
name|ontology
operator|.
name|OntologySpace
import|;
end_import

begin_import
import|import
name|eu
operator|.
name|iksproject
operator|.
name|kres
operator|.
name|api
operator|.
name|manager
operator|.
name|ontology
operator|.
name|OntologySpaceFactory
import|;
end_import

begin_import
import|import
name|eu
operator|.
name|iksproject
operator|.
name|kres
operator|.
name|api
operator|.
name|manager
operator|.
name|ontology
operator|.
name|ScopeRegistry
import|;
end_import

begin_import
import|import
name|eu
operator|.
name|iksproject
operator|.
name|kres
operator|.
name|api
operator|.
name|manager
operator|.
name|ontology
operator|.
name|UnmodifiableOntologySpaceException
import|;
end_import

begin_import
import|import
name|eu
operator|.
name|iksproject
operator|.
name|kres
operator|.
name|api
operator|.
name|manager
operator|.
name|session
operator|.
name|KReSSession
import|;
end_import

begin_import
import|import
name|eu
operator|.
name|iksproject
operator|.
name|kres
operator|.
name|api
operator|.
name|manager
operator|.
name|session
operator|.
name|KReSSessionManager
import|;
end_import

begin_import
import|import
name|eu
operator|.
name|iksproject
operator|.
name|kres
operator|.
name|api
operator|.
name|reasoners
operator|.
name|KReSReasoner
import|;
end_import

begin_import
import|import
name|eu
operator|.
name|iksproject
operator|.
name|kres
operator|.
name|api
operator|.
name|rules
operator|.
name|KReSRule
import|;
end_import

begin_import
import|import
name|eu
operator|.
name|iksproject
operator|.
name|kres
operator|.
name|api
operator|.
name|rules
operator|.
name|NoSuchRecipeException
import|;
end_import

begin_import
import|import
name|eu
operator|.
name|iksproject
operator|.
name|kres
operator|.
name|api
operator|.
name|rules
operator|.
name|Recipe
import|;
end_import

begin_import
import|import
name|eu
operator|.
name|iksproject
operator|.
name|kres
operator|.
name|api
operator|.
name|rules
operator|.
name|RuleStore
import|;
end_import

begin_import
import|import
name|eu
operator|.
name|iksproject
operator|.
name|kres
operator|.
name|api
operator|.
name|semion
operator|.
name|SemionManager
import|;
end_import

begin_import
import|import
name|eu
operator|.
name|iksproject
operator|.
name|kres
operator|.
name|api
operator|.
name|semion
operator|.
name|SemionRefactorer
import|;
end_import

begin_import
import|import
name|eu
operator|.
name|iksproject
operator|.
name|kres
operator|.
name|api
operator|.
name|semion
operator|.
name|SemionRefactoringException
import|;
end_import

begin_import
import|import
name|eu
operator|.
name|iksproject
operator|.
name|kres
operator|.
name|shared
operator|.
name|transformation
operator|.
name|OWLAPIToClerezzaConverter
import|;
end_import

begin_comment
comment|/**  *   * This an engine to post-process the FISE enhancements. Its main goal is to  * refactor the RDF produced by the enhancement applying some vocabulary related  * to a specific task.  *   * To do that, exploit a KReS recipe and an ontology scope.  *   * The first implementation is targeted to SEO use case. * It retrieves data by  * dereferencing the entities, * includes the DBpedia ontology * refactor the  * data using the google rich snippets vocabulary.  *   * @author andrea.nuzzolese  *   */
end_comment

begin_class
annotation|@
name|Component
argument_list|(
name|immediate
operator|=
literal|true
argument_list|,
name|metatype
operator|=
literal|true
argument_list|)
annotation|@
name|Service
argument_list|(
name|EnhancementEngine
operator|.
name|class
argument_list|)
specifier|public
class|class
name|Dulcifier
implements|implements
name|EnhancementEngine
implements|,
name|ServiceProperties
block|{
comment|/** 	 * TODO This are the scope and recipe IDs to be used by this implementation 	 * In future implementation this will be configurable 	 */
annotation|@
name|Property
argument_list|(
name|value
operator|=
literal|"http://fise.iks-project.eu/dulcifier"
argument_list|)
specifier|public
specifier|static
specifier|final
name|String
name|DULCIFIER_SCOPE
init|=
literal|"dulcifier.scope"
decl_stmt|;
annotation|@
name|Property
argument_list|(
name|value
operator|=
literal|"http://fise.iks-project.eu/dulcifier/recipe"
argument_list|)
specifier|public
specifier|static
specifier|final
name|String
name|DULCIFIER_RECIPE
init|=
literal|"dulcifier.recipe"
decl_stmt|;
annotation|@
name|Property
argument_list|(
name|value
operator|=
block|{
literal|"peopleTypeRule[is(dbpedia:Person%2C?x) -> is(google:Person%2C?x)]"
block|,
comment|//"myRule[has(fise:entity-reference%2C ?y%2C ?x) . has(<http://purl.org/dc/terms/relation>%2C ?y%2C ?r) ->  has(<http://purl.org/dc/terms/relation>%2C ?x%2C ?r)]",
literal|"fiseStartRul1[has(fise:entity-reference%2C ?y%2C ?x) . has(<http://purl.org/dc/terms/relation>%2C ?y%2C ?r) . values(fise:selected-text%2C ?r%2C ?t) . values(fise:start%2C ?r%2C ?start) -> is(fise:enhancementContext%2C ?t) . values(fise:start-position%2C ?r%2C ?start) . has(fise:hasEnhancementContext%2C ?x%2C ?r)]"
block|,
literal|"fiseEndRule1[has(fise:entity-reference%2C ?y%2C ?x) . has(<http://purl.org/dc/terms/relation>%2C ?y%2C ?r) . values(fise:selected-text%2C ?r%2C ?t) . values(fise:end%2C ?r%2C ?end) -> is(fise:enhancementContext%2C ?t) . values(fise:end-position%2C ?r%2C ?end) . has(fise:hasEnhancementContext%2C ?x%2C ?r) ]"
block|,
literal|"fiseContextRule1[has(fise:entity-reference%2C ?y%2C ?x) . has(<http://purl.org/dc/terms/relation>%2C ?y%2C ?r) . values(fise:selected-text%2C ?r%2C ?t) . values(fise:selection-context%2C ?r%2C ?context) -> is(fise:enhancementContext%2C ?t) . values(fise:context%2C ?r%2C ?context) . has(fise:hasEnhancementContext%2C ?x%2C ?r)]"
block|,
comment|//"fiseStartRul[has(fise:entity-reference%2C ?y%2C ?x) . values(fise:entity-label%2C ?y%2C ?z) . values(fise:selected-text%2C ?t%2C ?z) . values(fise:start%2C ?t%2C ?start) -> is(fise:enhancementContext%2C ?t) . values(fise:start-position%2C ?t%2C ?start) . has(fise:hasEnhancementContext%2C ?x%2C ?t)]",
comment|//"fiseEndRule[has(fise:entity-reference%2C ?y%2C ?x) . values(fise:entity-label%2C ?y%2C ?z) . values(fise:selected-text%2C ?t%2C ?z) . values(fise:end%2C ?t%2C ?end) -> is(fise:enhancementContext%2C ?t) . values(fise:end-position%2C ?t%2C ?end) . has(fise:hasEnhancementContext%2C ?x%2C ?t) ]",
comment|//"fiseContextRule[has(fise:entity-reference%2C ?y%2C ?x) . values(fise:entity-label%2C ?y%2C ?z) . values(fise:selected-text%2C ?t%2C ?z) . values(fise:selection-context%2C ?t%2C ?context) -> is(fise:enhancementContext%2C ?t) . values(fise:context%2C ?t%2C ?context) . has(fise:hasEnhancementContext%2C ?x%2C ?t)]",
literal|"peopleNameRule[is(dbpedia:Person%2C?x) . values(foaf:name%2C?x%2C?y) -> values(google:name%2C?x%2C?y)]"
block|,
literal|"peopleNickRule[is(dbpedia:Person%2C?x) . values(foaf:nick%2C?x%2C?y) -> values(google:nickname%2C?x%2C?y)]"
block|,
literal|"peoplePhotoRule[is(dbpedia:Person%2C?x) . has(dbpedia:thumbnail%2C?x%2C?y) -> has(google:photo%2C?x%2C?y)]"
block|,
literal|"peopleProfessionRule[is(dbpedia:Person%2C?x) . has(dbpedia:profession%2C?x%2C?y) -> has(google:title%2C?x%2C?y)]"
block|,
literal|"peopleOccupationRule[is(dbpedia:Person%2C?x) . has(dbpedia:occupation%2C?x%2C?y) -> has(google:title%2C?x%2C?y)]"
block|,
literal|"peopleRoleRule[is(dbpedia:Person%2C?x) . values(dbpedia:role%2C?x%2C?y) -> values(google:role%2C?x%2C?y)]"
block|,
literal|"peopleHomepageRule[is(dbpedia:Person%2C?x) . has(foaf:homepage%2C?x%2C?y) -> has(google:url%2C?x%2C?y)]"
block|,
literal|"peopleAffiliationRule[is(dbpedia:Person%2C?x) . has(dbpedia:employer%2C?x%2C?y) -> has(google:affiliation%2C?x%2C?y)]"
block|,
literal|"peopleKnowsRule[is(dbpedia:Person%2C?x) . has(foaf:knows%2C?x%2C?y) -> has(google:friend%2C?x%2C?y)]"
block|,
literal|"peopleAddressRule[is(dbpedia:Person%2C?x) . values(dbpedia:address%2C?x%2C?y) -> values(google:address%2C?x%2C?y)]"
block|,
literal|"peopleOccupationRule2[is(dbpedia:Person%2C?x) . has(dc:description%2C?x%2C?y) -> has(google:title%2C?x%2C?y)]"
block|,
literal|"peopleOccupationRule3[is(dbpedia:Person%2C?x) . has(skos:subject%2C?x%2C?y) -> has(google:affiliation%2C?x%2C?y)]"
block|,
literal|"productTypeRule[is(dbpedia:Organisation%2C?x) . has(dbpedia:product%2C?x%2C?y) -> is(google:Product%2C?y)]"
block|,
literal|"productNameRule1[is(dbpedia:Organisation%2C?x) . has(dbpedia:product%2C?x%2C?y) . values(foaf:name%2C?y%2C?z) -> values(google:name%2C?y%2C?z)]"
block|,
literal|"productNameRule2[is(dbpedia:Organisation%2C?x) . has(dbpedia:product%2C?x%2C?y) . values(dbprop:name%2C?y%2C?z) -> values(google:name%2C?y%2C?z)]"
block|,
literal|"productNameRule3[is(dbpedia:Organisation%2C?x) . has(dbpedia:product%2C?x%2C?y) . values(rdf:label%2C?y%2C?z) -> values(google:name%2C?y%2C?z)]"
block|,
literal|"productImageRule[is(dbpedia:Organisation%2C?x) . has(dbpedia:product%2C?x%2C?y) . has(dbpedia:thumbnail%2C?y%2C?z) -> has(google:photo%2C?y%2C?z)]"
block|,
literal|"productDescriptionRule[is(dbpedia:Organisation%2C?x) . has(dbpedia:product%2C?x%2C?y) . values(dbpedia:thumbnail%2C?y%2C?z) -> values(google:description%2C?y%2C?z)]"
block|,
literal|"productBrandRule[is(dbpedia:Organisation%2C?x) . has(dbpedia:product%2C?x%2C?y) . values(rdf:label%2C?y%2C?z) -> values(google:brand%2C?y%2C?z)]"
block|,
literal|"productIdentifierRule[is(dbpedia:Organisation%2C?x) . has(dbpedia:product%2C?x%2C?y) . values(dbpedia:isbn%2C?y%2C?z) -> values(google:identifier%2C?y%2C?z)]"
block|,
literal|"productHomepageRule[is(dbpedia:Organisation%2C?x) . has(dbpedia:product%2C?x%2C?y) . values(foaf:homepage%2C?y%2C?z) -> values(google:url%2C?y%2C?z)]"
block|,
literal|"productCategoryRule[is(dbpedia:Organisation%2C?x) . has(dbpedia:product%2C?x%2C?y) . has(skos:currency%2C?y%2C?z) -> has(google:category%2C?y%2C?z)]"
block|,
literal|"organizationTypeRule[is(dbpedia:Organisation%2C?x) -> is(google:Organization%2C?x)]"
block|,
literal|"organizationNameRule[is(dbpedia:Organisation%2C?x) . values(foaf:name%2C?x%2C?y) -> values(google:name%2C?x%2C?y)]"
block|,
literal|"organizationHomepageRule[is(dbpedia:Organisation%2C?x) . values(foaf:homepage%2C?x%2C?y) -> values(google:url%2C?x%2C?y)]"
block|,
literal|"organizationRegionRule[is(dbpedia:Organisation%2C?x) . has(dbpedia:region%2C?x%2C?y) -> has(google:region%2C?x%2C?y)]"
block|,
literal|"organizationCountryRule[is(dbpedia:Organisation%2C?x) . has(dbpedia:locationCountry%2C?x%2C?y) -> has(google:country-name%2C?x%2C?y)]"
block|,
literal|"organizationAddressRule[is(dbpedia:Organisation%2C?x) . values(dbprop:address%2C?x%2C?y) -> values(google:address%2C?x%2C?y)]"
block|,
literal|"organizationStreetAddressRule[is(dbpedia:Organisation%2C?x) . values(dbprop:streetaddress%2C?x%2C?y) -> values(google:street-address%2C?x%2C?y)]"
block|,
literal|"organizationLocationRule[is(dbpedia:Organisation%2C?x) . has(dbpedia:location%2C?x%2C?y) -> has(google:locality%2C?x%2C?y)]"
block|,
literal|"organizationTelephoneRule[is(dbpedia:Organisation%2C?x) . values(dbprop:telephon%2C?x%2C?y) -> values(google:tel%2C?x%2C?y)]"
block|,
literal|"organizationPostalCodeRule[is(dbpedia:Organisation%2C?x) . values(dbpedia:postalCode%2C?x%2C?y) -> has(google:postal-code%2C?x%2C?y)]"
block|,
literal|"organizationGeoLatRule[is(dbpedia:Organisation%2C?x) . values(gn:lat%2C?x%2C?y) -> values(google:latitude%2C?x%2C?y)]"
block|,
literal|"organizationGeoLongRule[is(dbpedia:Organisation%2C?x) . values(gn:long%2C?x%2C?y) -> values(google:longitude%2C?x%2C?y)]"
block|,
literal|"organizationCategoryRule[is(dbpedia:Organisation%2C?x) . has(skos:subject%2C?x%2C?y) -> has(google:category%2C?x%2C?y)]"
block|,
literal|"eventTypeRule[is(dbpedia:Event%2C?x) -> is(google:Event%2C?x)]"
block|,
literal|"eventURLRule[is(dbpedia:Event%2C?x) . has(foaf:page%2C?x%2C?y) -> has(google:url%2C?x%2C?y)]"
block|,
literal|"eventLocationRule1[is(dbpedia:Event%2C?x) . has(dbpedia:place%2C?x%2C?y) -> has(google:location%2C?x%2C?y)]"
block|,
literal|"eventLocationRule2[is(dbpedia:Event%2C?x) . has(dbpedia:place%2C?x%2C?y) . has(owl:sameAs%2C?y%2C?z) . is(gn:Feature%2C?z) . values(wgs84_pos:lat%2C?z%2C?lat) . values(wgs84_pos:long%2C?z%2C?long) -> is(google:geo%2C?z) . has(google:location%2C?x%2C?y) . has(google:geo%2C?y%2C?z) . values(google:latitude%2C?z%2C?lat) . values(google:longitude%2C?z%2C?long)]"
block|,
literal|"eventDateRule1[is(dbpedia:Event%2C?x) . values(dbpedia:date%2C?x%2C?y) -> values(google:startDate%2C?x%2C?y)]"
block|,
literal|"eventCategoryRule[is(dbpedia:Event%2C?x) . has(skos:subject%2C?x%2C?y) -> has(google:eventType%2C?x%2C?y)]"
block|,
literal|"eventPhotoRule[is(dbpedia:Event%2C?x) . has(dbpedia:thumbnail%2C?x%2C?y) -> has(google:photo%2C?x%2C?y)]"
block|,
literal|"recipeClassAssertionRule[has(skos:subject%2C?x%2C<http://dbpedia.org/page/Category:World_cuisine>) -> is(google:Recipe%2C?x)]"
block|,
literal|"recipeTypeRule[has(skos:subject%2C?x%2C<http://dbpedia.org/page/Category:World_cuisine>) . has(skos:subject%2C?x%2C?y) -> has(google:recipeType%2C?x%2C?y)]"
block|,
literal|"recipePhotoRule1[has(skos:subject%2C?x%2C<http://dbpedia.org/page/Category:World_cuisine>) . has(dbpedia:thumbnail%2C?x%2C?y) -> has(google:photo%2C?x%2C?y)]"
block|,
literal|"recipePhotoRule2[has(skos:subject%2C?x%2C<http://dbpedia.org/page/Category:World_cuisine>) . values(dbpedia:abstract%2C?x%2C?y) -> values(google:summary%2C?x%2C?y)]"
block|}
argument_list|,
name|cardinality
operator|=
literal|1000
argument_list|,
name|description
operator|=
literal|"Rule/s specified in kReS syntax. This/these rule/s belong to the specified recipe"
argument_list|)
specifier|public
specifier|static
specifier|final
name|String
name|DULCIFIER_RECIPE_RULE
init|=
literal|"dulcifier.recipe.rule"
decl_stmt|;
annotation|@
name|Property
argument_list|(
name|value
operator|=
block|{
literal|"dbpedia =<http://dbpedia.org/ontology/>"
block|,
literal|"dbprop =<http://dbpedia.org/property/>"
block|,
literal|"google =<http://rdf.data-vocabulary.org/#>"
block|,
literal|"foaf =<http://xmlns.com/foaf/0.1/>"
block|,
literal|"rdf =<http://www.w3.org/1999/02/22-rdf-syntax-ns#>"
block|,
literal|"wgs84_pos =<http://www.w3.org/2003/01/geo/wgs84_pos#>"
block|,
literal|"skos =<http://www.w3.org/2004/02/skos/core#>"
block|,
literal|"gn =<http://www.geonames.org/ontology#>"
block|,
literal|"fise =<http://fise.iks-project.eu/ontology/>"
block|,
literal|"owl =<http://www.w3.org/2002/07/owl#>"
block|,
literal|"dc =<http://purl.org/dc/elements/1.1/>"
block|}
argument_list|,
name|cardinality
operator|=
literal|1000
argument_list|,
name|description
operator|=
literal|"Base prefix to be used for the rules."
argument_list|)
specifier|public
specifier|static
specifier|final
name|String
name|DULCIFIER_RECIPE_RULE_PREFIX
init|=
literal|"dulcifier.recipe.rule.prefix"
decl_stmt|;
annotation|@
name|Property
argument_list|(
name|value
operator|=
block|{
literal|"http://ontologydesignpatterns.org/ont/iks/kres/dbpedia_demo.owl"
block|,
literal|""
block|}
argument_list|,
name|cardinality
operator|=
literal|1000
argument_list|,
name|description
operator|=
literal|"To fix a set of resolvable ontology URIs for the scope's ontologies."
argument_list|)
specifier|public
specifier|static
specifier|final
name|String
name|DULCIFIER_SCOPE_CORE_ONTOLOGY
init|=
literal|"dulcifier.scope.core.ontology"
decl_stmt|;
annotation|@
name|Property
argument_list|(
name|value
operator|=
literal|"true"
argument_list|,
name|description
operator|=
literal|"If true: the previously generated RDF is deleted and substituted with the new one. If false: the new one is appended to the old RDF. Possible value: true or false."
argument_list|)
specifier|public
specifier|static
specifier|final
name|String
name|DULCIFIER_APPEND
init|=
literal|"dulcifier.append"
decl_stmt|;
annotation|@
name|Reference
name|KReSONManager
name|onManager
decl_stmt|;
annotation|@
name|Reference
name|IDereferencer
name|dereferencer
decl_stmt|;
annotation|@
name|Reference
name|RuleStore
name|ruleStore
decl_stmt|;
annotation|@
name|Reference
name|KReSReasoner
name|reasoner
decl_stmt|;
annotation|@
name|Reference
name|SemionManager
name|semion
decl_stmt|;
specifier|private
name|OntologyScope
name|scope
decl_stmt|;
specifier|private
name|IRI
name|recipeIRI
decl_stmt|;
specifier|private
name|boolean
name|graph_append
decl_stmt|;
specifier|private
specifier|final
name|Logger
name|log
init|=
name|LoggerFactory
operator|.
name|getLogger
argument_list|(
name|getClass
argument_list|()
argument_list|)
decl_stmt|;
annotation|@
name|Override
specifier|public
name|int
name|canEnhance
parameter_list|(
name|ContentItem
name|ci
parameter_list|)
throws|throws
name|EngineException
block|{
comment|/* 		 * Dulcifier can enhance only content items that are previously enhanced 		 * by other FISE engines, as it must be the last engine in the chain. 		 *  		 * Works only if some enhancement has been produced. 		 */
name|MGraph
name|mGraph
init|=
name|ci
operator|.
name|getMetadata
argument_list|()
decl_stmt|;
if|if
condition|(
name|mGraph
operator|!=
literal|null
condition|)
block|{
return|return
name|ENHANCE_SYNCHRONOUS
return|;
block|}
else|else
block|{
return|return
name|CANNOT_ENHANCE
return|;
block|}
block|}
annotation|@
name|Override
specifier|public
name|void
name|computeEnhancements
parameter_list|(
name|ContentItem
name|ci
parameter_list|)
throws|throws
name|EngineException
block|{
comment|/** 		 * Retrieve the graph 		 */
specifier|final
name|MGraph
name|mGraph
init|=
name|ci
operator|.
name|getMetadata
argument_list|()
decl_stmt|;
comment|/** 		 * We filter the entities recognized by the engines 		 */
name|UriRef
name|fiseEntityReference
init|=
operator|new
name|UriRef
argument_list|(
literal|"http://fise.iks-project.eu/ontology/entity-reference"
argument_list|)
decl_stmt|;
name|Iterator
argument_list|<
name|Triple
argument_list|>
name|tripleIt
init|=
name|mGraph
operator|.
name|filter
argument_list|(
literal|null
argument_list|,
name|fiseEntityReference
argument_list|,
literal|null
argument_list|)
decl_stmt|;
comment|/** 		 * Now we prepare the KreS environment. First we create the kres session 		 * in which run the whole 		 */
specifier|final
name|IRI
name|sessionIRI
init|=
name|createAndAddSessionSpaceToScope
argument_list|()
decl_stmt|;
comment|/** 		 * Now we fetch any single entity by dereferencing the URI and we add 		 * the returned RDF to our session-ontology 		 */
comment|/** 		 * We retrieve the session space 		 */
name|OntologySpace
name|sessionSpace
init|=
name|scope
operator|.
name|getSessionSpace
argument_list|(
name|sessionIRI
argument_list|)
decl_stmt|;
while|while
condition|(
name|tripleIt
operator|.
name|hasNext
argument_list|()
condition|)
block|{
name|Triple
name|triple
init|=
name|tripleIt
operator|.
name|next
argument_list|()
decl_stmt|;
name|Resource
name|entityReference
init|=
name|triple
operator|.
name|getObject
argument_list|()
decl_stmt|;
comment|/** 			 * the entity uri 			 */
specifier|final
name|String
name|entityReferenceString
init|=
name|entityReference
operator|.
name|toString
argument_list|()
operator|.
name|replace
argument_list|(
literal|"<"
argument_list|,
literal|""
argument_list|)
operator|.
name|replace
argument_list|(
literal|">"
argument_list|,
literal|""
argument_list|)
decl_stmt|;
name|log
operator|.
name|debug
argument_list|(
literal|"Trying to resolve entity "
operator|+
name|entityReferenceString
argument_list|)
expr_stmt|;
comment|/** 			 * We fetch the entity in the OntologyInputSource object 			 */
try|try
block|{
specifier|final
name|IRI
name|fetchedIri
init|=
name|IRI
operator|.
name|create
argument_list|(
name|entityReferenceString
argument_list|)
decl_stmt|;
name|OWLOntologyManager
name|manager
init|=
name|OWLManager
operator|.
name|createOWLOntologyManager
argument_list|()
decl_stmt|;
specifier|final
name|OWLOntology
name|fetched
init|=
name|manager
operator|.
name|loadOntologyFromOntologyDocument
argument_list|(
name|dereferencer
operator|.
name|resolve
argument_list|(
name|entityReferenceString
argument_list|)
argument_list|)
decl_stmt|;
name|OntologyInputSource
name|ontologySource
init|=
operator|new
name|OntologyInputSource
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|boolean
name|hasRootOntology
parameter_list|()
block|{
return|return
operator|(
name|fetched
operator|!=
literal|null
operator|)
return|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|hasPhysicalIRI
parameter_list|()
block|{
return|return
literal|true
return|;
block|}
annotation|@
name|Override
specifier|public
name|OWLOntology
name|getRootOntology
parameter_list|()
block|{
return|return
name|fetched
return|;
block|}
annotation|@
name|Override
specifier|public
name|IRI
name|getPhysicalIRI
parameter_list|()
block|{
return|return
name|fetchedIri
return|;
block|}
block|}
decl_stmt|;
name|sessionSpace
operator|.
name|addOntology
argument_list|(
name|ontologySource
argument_list|)
expr_stmt|;
name|log
operator|.
name|debug
argument_list|(
literal|"Added "
operator|+
name|entityReferenceString
operator|+
literal|" to the session space of scope "
operator|+
name|scope
operator|.
name|getID
argument_list|()
operator|.
name|toString
argument_list|()
argument_list|,
name|this
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|OWLOntologyCreationException
name|e1
parameter_list|)
block|{
name|log
operator|.
name|error
argument_list|(
literal|"Cannot load the entity"
argument_list|,
name|e1
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|FileNotFoundException
name|e1
parameter_list|)
block|{
name|log
operator|.
name|error
argument_list|(
literal|"Cannot load the entity"
argument_list|,
name|e1
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|UnmodifiableOntologySpaceException
name|e
parameter_list|)
block|{
name|log
operator|.
name|error
argument_list|(
literal|"Cannot load the entity"
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
block|}
comment|/** 		 * Now we merge the RDF from the T-box - the ontologies - and the A-box 		 * - the RDF data fetched 		 *  		 * FIXME TODO This results in a quite huge amunt of code. Since it is a 		 * very common operation for KReS clients, this should be moved to the 		 * KReS API. 		 */
specifier|final
name|OWLOntologyManager
name|man
init|=
name|OWLManager
operator|.
name|createOWLOntologyManager
argument_list|()
decl_stmt|;
name|OWLOntologySetProvider
name|provider
init|=
operator|new
name|OWLOntologySetProvider
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|Set
argument_list|<
name|OWLOntology
argument_list|>
name|getOntologies
parameter_list|()
block|{
comment|/** 				 * We need to remove data property assertions, since 				 */
name|Set
argument_list|<
name|OWLOntology
argument_list|>
name|ontologies
init|=
operator|new
name|HashSet
argument_list|<
name|OWLOntology
argument_list|>
argument_list|()
decl_stmt|;
name|OntologySpace
name|sessionSpace
init|=
name|scope
operator|.
name|getSessionSpace
argument_list|(
name|sessionIRI
argument_list|)
decl_stmt|;
name|ontologies
operator|.
name|addAll
argument_list|(
name|sessionSpace
operator|.
name|getOntologies
argument_list|()
argument_list|)
expr_stmt|;
comment|/** 				 * We add to the set the graph containing the metadata generated by previous 				 * enhancement engines. It is important becaus we want to menage during the refactoring 				 * also some information fron that graph. 				 * As the graph is provided as a Clerezza MGraph, we first need to convert it to an OWLAPI 				 * OWLOntology. 				 * There is no chance that the mGraph could be null as it was previously controlled by the JobManager 				 * through the canEnhance method and the computeEnhancement is always called iff the former returns true.   				 */
name|OWLOntology
name|fiseMetadataOntology
init|=
name|OWLAPIToClerezzaConverter
operator|.
name|clerezzaMGraphToOWLOntology
argument_list|(
name|mGraph
argument_list|)
decl_stmt|;
name|ontologies
operator|.
name|add
argument_list|(
name|fiseMetadataOntology
argument_list|)
expr_stmt|;
return|return
name|ontologies
return|;
block|}
block|}
decl_stmt|;
comment|/** 		 * We merge all the ontologies from the session space of the scope into 		 * a single ontology that will be used for the refactoring. 		 */
name|OWLOntologyMerger
name|merger
init|=
operator|new
name|OWLOntologyMerger
argument_list|(
name|provider
argument_list|)
decl_stmt|;
name|OWLOntology
name|ontology
decl_stmt|;
try|try
block|{
name|ontology
operator|=
name|merger
operator|.
name|createMergedOntology
argument_list|(
name|man
argument_list|,
name|IRI
operator|.
name|create
argument_list|(
literal|"http://fise.iks-project.eu/dulcifier/integrity-check"
argument_list|)
argument_list|)
expr_stmt|;
comment|/** 			 * To perform the refactoring of the ontology to the google 			 * vocabulary we need to get the instance of the refactorer through 			 * the Semion Manager. 			 */
name|SemionRefactorer
name|refactorer
init|=
name|semion
operator|.
name|getRegisteredRefactorer
argument_list|()
decl_stmt|;
name|log
operator|.
name|debug
argument_list|(
literal|"Refactoring recipe IRI is : "
operator|+
name|recipeIRI
argument_list|)
expr_stmt|;
comment|/** 			 * We pass the ontology and the recipe IRI to the refactorer that 			 * returns the refactorer ontology expressed by using the google 			 * vocabulary. 			 */
try|try
block|{
name|Recipe
name|recipe
init|=
name|ruleStore
operator|.
name|getRecipe
argument_list|(
name|recipeIRI
argument_list|)
decl_stmt|;
name|log
operator|.
name|debug
argument_list|(
literal|"Rules in the recipe are : "
operator|+
name|recipe
operator|.
name|getkReSRuleList
argument_list|()
operator|.
name|size
argument_list|()
argument_list|,
name|this
argument_list|)
expr_stmt|;
name|log
operator|.
name|debug
argument_list|(
literal|"The ontology to be refactor is : "
operator|+
name|ontology
argument_list|,
name|this
argument_list|)
expr_stmt|;
name|ontology
operator|=
name|refactorer
operator|.
name|ontologyRefactoring
argument_list|(
name|ontology
argument_list|,
name|recipeIRI
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|SemionRefactoringException
name|e
parameter_list|)
block|{
name|log
operator|.
name|error
argument_list|(
literal|"The refactoring engine failed the execution."
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|NoSuchRecipeException
name|e
parameter_list|)
block|{
name|log
operator|.
name|error
argument_list|(
literal|"The recipe with ID "
operator|+
name|recipeIRI
operator|+
literal|" does not exists"
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
name|log
operator|.
name|debug
argument_list|(
literal|"Merged ontologies in "
operator|+
name|ontology
argument_list|)
expr_stmt|;
comment|/**          	 * The new generated ontology is converted to Clarezza format and than added os substitued to the old mGraph.          	 */
if|if
condition|(
name|graph_append
condition|)
block|{
name|mGraph
operator|.
name|addAll
argument_list|(
name|OWLAPIToClerezzaConverter
operator|.
name|owlOntologyToClerezzaTriples
argument_list|(
name|ontology
argument_list|)
argument_list|)
expr_stmt|;
name|log
operator|.
name|debug
argument_list|(
literal|"Metadata of the content passd have been substituted"
argument_list|,
name|this
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|mGraph
operator|.
name|removeAll
argument_list|(
name|mGraph
argument_list|)
expr_stmt|;
name|mGraph
operator|.
name|addAll
argument_list|(
name|OWLAPIToClerezzaConverter
operator|.
name|owlOntologyToClerezzaTriples
argument_list|(
name|ontology
argument_list|)
argument_list|)
expr_stmt|;
name|log
operator|.
name|debug
argument_list|(
literal|"Metadata of the content is appended to the existent one"
argument_list|,
name|this
argument_list|)
expr_stmt|;
block|}
comment|/** 			 * The session needs to be destroyed, as it is no more useful. 			 */
name|onManager
operator|.
name|getSessionManager
argument_list|()
operator|.
name|destroySession
argument_list|(
name|sessionIRI
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|OWLOntologyCreationException
name|e
parameter_list|)
block|{
name|log
operator|.
name|error
argument_list|(
literal|"Cannot create the ontology for the refactoring"
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
block|}
comment|/** 	 * Setup the KReS session 	 *  	 * @return 	 */
specifier|private
name|IRI
name|createAndAddSessionSpaceToScope
parameter_list|()
block|{
comment|/** 		 * Retrieve the session manager 		 */
name|KReSSessionManager
name|sessionManager
init|=
name|onManager
operator|.
name|getSessionManager
argument_list|()
decl_stmt|;
name|log
operator|.
name|debug
argument_list|(
literal|"Starting create session for the dulcifier"
argument_list|)
expr_stmt|;
comment|/* 		 * Create and setup the session. TODO FIXME This is an operation that 		 * should be made easier for developers to do through the API 		 */
name|KReSSession
name|session
init|=
name|sessionManager
operator|.
name|createSession
argument_list|()
decl_stmt|;
name|OntologySpaceFactory
name|ontologySpaceFactory
init|=
name|onManager
operator|.
name|getOntologySpaceFactory
argument_list|()
decl_stmt|;
name|OntologySpace
name|sessionSpace
init|=
name|ontologySpaceFactory
operator|.
name|createSessionOntologySpace
argument_list|(
name|scope
operator|.
name|getID
argument_list|()
argument_list|)
decl_stmt|;
name|scope
operator|.
name|addSessionSpace
argument_list|(
name|sessionSpace
argument_list|,
name|session
operator|.
name|getID
argument_list|()
argument_list|)
expr_stmt|;
comment|/** 		 * Finally, we return the session ID to be used by the caller 		 */
name|log
operator|.
name|debug
argument_list|(
literal|"Session "
operator|+
name|session
operator|.
name|getID
argument_list|()
operator|+
literal|" created"
argument_list|,
name|this
argument_list|)
expr_stmt|;
return|return
name|session
operator|.
name|getID
argument_list|()
return|;
block|}
comment|/**      * To create the input source necesary to load the ontology inside the scope.      * @param uri -- A resolvable string uri.      * @return An OntologyInputSource      */
specifier|private
name|OntologyInputSource
name|oisForScope
parameter_list|(
specifier|final
name|String
name|uri
parameter_list|)
block|{
comment|/* 		 * The scope factory needs an OntologyInputSource as input for the core 		 * ontology space. We want to use the dbpedia ontology as core ontology 		 * of our scope. 		 */
name|OntologyInputSource
name|ois
init|=
operator|new
name|OntologyInputSource
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|boolean
name|hasRootOntology
parameter_list|()
block|{
return|return
literal|true
return|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|hasPhysicalIRI
parameter_list|()
block|{
return|return
literal|false
return|;
block|}
annotation|@
name|Override
specifier|public
name|OWLOntology
name|getRootOntology
parameter_list|()
block|{
name|InputStream
name|inputStream
decl_stmt|;
try|try
block|{
comment|/* 					 * The input stream for the dbpedia ontology is obtained 					 * through the dereferencer component. 					 */
name|inputStream
operator|=
name|dereferencer
operator|.
name|resolve
argument_list|(
name|uri
argument_list|)
expr_stmt|;
name|OWLOntologyManager
name|manager
init|=
name|OWLManager
operator|.
name|createOWLOntologyManager
argument_list|()
decl_stmt|;
return|return
name|manager
operator|.
name|loadOntologyFromOntologyDocument
argument_list|(
name|inputStream
argument_list|)
return|;
block|}
catch|catch
parameter_list|(
name|FileNotFoundException
name|e
parameter_list|)
block|{
name|log
operator|.
name|error
argument_list|(
literal|"Cannot load the ontology "
operator|+
name|uri
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|OWLOntologyCreationException
name|e
parameter_list|)
block|{
name|log
operator|.
name|error
argument_list|(
literal|"Cannot load the ontology "
operator|+
name|uri
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|log
operator|.
name|error
argument_list|(
literal|"Cannot load the ontology "
operator|+
name|uri
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
comment|/** If some errors occur **/
return|return
literal|null
return|;
block|}
annotation|@
name|Override
specifier|public
name|IRI
name|getPhysicalIRI
parameter_list|()
block|{
return|return
literal|null
return|;
block|}
block|}
decl_stmt|;
return|return
name|ois
return|;
block|}
comment|/**          * Activating the component          * @param context          */
specifier|protected
name|void
name|activate
parameter_list|(
name|ComponentContext
name|context
parameter_list|)
block|{
comment|/**      	 * Read property to indicate if the the new eanchment metada must be append to the existing mGraph       	 */
name|graph_append
operator|=
name|Boolean
operator|.
name|parseBoolean
argument_list|(
operator|(
operator|(
name|String
operator|)
name|context
operator|.
name|getProperties
argument_list|()
operator|.
name|get
argument_list|(
name|DULCIFIER_APPEND
argument_list|)
operator|)
operator|.
name|toLowerCase
argument_list|()
argument_list|)
expr_stmt|;
comment|/** 		 * Get the Scope Factory from the ONM of KReS that allows to create new 		 * scopes 		 */
name|OntologyScopeFactory
name|scopeFactory
init|=
name|onManager
operator|.
name|getOntologyScopeFactory
argument_list|()
decl_stmt|;
comment|/**     	 * Adding ontologies to the scope core ontology.     	 * 1) Get all the ontologies from the property.     	 * 2) Create a base scope with an empity ontology.     	 * 3) Retrieve the ontology space from the scope.      	 * 4) Add the ontologies to the scope via ontology space.      	 */
comment|//Step 1
name|Object
name|obj
init|=
name|context
operator|.
name|getProperties
argument_list|()
operator|.
name|get
argument_list|(
name|DULCIFIER_SCOPE_CORE_ONTOLOGY
argument_list|)
decl_stmt|;
name|String
index|[]
name|coreScopeOntologySet
decl_stmt|;
if|if
condition|(
name|obj
operator|instanceof
name|String
index|[]
condition|)
block|{
name|coreScopeOntologySet
operator|=
operator|(
name|String
index|[]
operator|)
name|obj
expr_stmt|;
block|}
else|else
block|{
name|String
index|[]
name|aux
init|=
operator|new
name|String
index|[
literal|1
index|]
decl_stmt|;
name|aux
index|[
literal|0
index|]
operator|=
operator|(
name|String
operator|)
name|obj
expr_stmt|;
name|coreScopeOntologySet
operator|=
name|aux
expr_stmt|;
block|}
comment|//Step 2
name|OntologyInputSource
name|oisbase
init|=
operator|new
name|OntologyInputSource
argument_list|()
block|{
annotation|@
name|Override
specifier|public
name|boolean
name|hasRootOntology
parameter_list|()
block|{
return|return
literal|true
return|;
block|}
annotation|@
name|Override
specifier|public
name|boolean
name|hasPhysicalIRI
parameter_list|()
block|{
return|return
literal|false
return|;
block|}
annotation|@
name|Override
specifier|public
name|OWLOntology
name|getRootOntology
parameter_list|()
block|{
try|try
block|{
comment|/* 					 * The input stream for the dbpedia ontology is obtained 					 * through the dereferencer component. 					 */
name|OWLOntologyManager
name|manager
init|=
name|OWLManager
operator|.
name|createOWLOntologyManager
argument_list|()
decl_stmt|;
return|return
name|manager
operator|.
name|createOntology
argument_list|()
return|;
block|}
catch|catch
parameter_list|(
name|OWLOntologyCreationException
name|e
parameter_list|)
block|{
name|log
operator|.
name|error
argument_list|(
literal|"Cannot create the scope with empity ontology."
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|Exception
name|e
parameter_list|)
block|{
name|log
operator|.
name|error
argument_list|(
literal|"Cannot create the scope with empity ontology."
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
comment|/** If some errors occur **/
return|return
literal|null
return|;
block|}
annotation|@
name|Override
specifier|public
name|IRI
name|getPhysicalIRI
parameter_list|()
block|{
return|return
literal|null
return|;
block|}
block|}
decl_stmt|;
name|IRI
name|dulcifierScopeIRI
init|=
name|IRI
operator|.
name|create
argument_list|(
operator|(
name|String
operator|)
name|context
operator|.
name|getProperties
argument_list|()
operator|.
name|get
argument_list|(
name|DULCIFIER_SCOPE
argument_list|)
argument_list|)
decl_stmt|;
comment|/** 		 * The scope is created by the ScopeFactory or loaded from the scope 		 * registry of KReS 		 */
try|try
block|{
name|scope
operator|=
name|scopeFactory
operator|.
name|createOntologyScope
argument_list|(
name|dulcifierScopeIRI
argument_list|,
name|oisbase
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|DuplicateIDException
name|e
parameter_list|)
block|{
name|ScopeRegistry
name|scopeRegistry
init|=
name|onManager
operator|.
name|getScopeRegistry
argument_list|()
decl_stmt|;
name|scope
operator|=
name|scopeRegistry
operator|.
name|getScope
argument_list|(
name|dulcifierScopeIRI
argument_list|)
expr_stmt|;
block|}
comment|/**          * Step 3          */
name|OntologySpace
name|ontologySpace
init|=
name|scope
operator|.
name|getCoreSpace
argument_list|()
decl_stmt|;
comment|/**          * Step 4          */
name|ontologySpace
operator|.
name|tearDown
argument_list|()
expr_stmt|;
for|for
control|(
name|int
name|o
init|=
literal|0
init|;
name|o
operator|<
name|coreScopeOntologySet
operator|.
name|length
condition|;
name|o
operator|++
control|)
block|{
name|OntologyInputSource
name|ois
init|=
name|oisForScope
argument_list|(
name|coreScopeOntologySet
index|[
name|o
index|]
argument_list|)
decl_stmt|;
try|try
block|{
name|ontologySpace
operator|.
name|addOntology
argument_list|(
name|ois
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|UnmodifiableOntologySpaceException
name|ex
parameter_list|)
block|{
name|log
operator|.
name|error
argument_list|(
literal|"Unmodifiable Ontology SpaceException."
argument_list|,
name|ex
argument_list|)
expr_stmt|;
block|}
block|}
name|ontologySpace
operator|.
name|setUp
argument_list|()
expr_stmt|;
name|log
operator|.
name|debug
argument_list|(
literal|"The set of ontologies loaded in the core scope space is: "
operator|+
name|ontologySpace
operator|.
name|getOntologies
argument_list|()
operator|+
literal|"\nN.B. The root.owl ontology is the first (on the list) ontology added when the scope is created."
argument_list|)
expr_stmt|;
comment|/**           * Set the recipe:          * 1) Create the recipe          * 2) Set the rule          *  2.1) Gathering the base prefixes and put them in a single string to be added to the rule.          *  2.2) Gathering the rules and put them in a single string to be added to the recipe.          * 3) Add rule to the recipe          */
comment|/**          * step 1          */
name|recipeIRI
operator|=
name|IRI
operator|.
name|create
argument_list|(
operator|(
name|String
operator|)
name|context
operator|.
name|getProperties
argument_list|()
operator|.
name|get
argument_list|(
name|DULCIFIER_RECIPE
argument_list|)
argument_list|)
expr_stmt|;
name|log
operator|.
name|debug
argument_list|(
literal|"Start create the Recipe"
argument_list|,
name|this
argument_list|)
expr_stmt|;
name|ruleStore
operator|.
name|addRecipe
argument_list|(
name|recipeIRI
argument_list|,
literal|null
argument_list|)
expr_stmt|;
name|log
operator|.
name|debug
argument_list|(
literal|"The recipe has been created"
argument_list|,
name|this
argument_list|)
expr_stmt|;
comment|/**          * step 2          *          * Set the rules. The multiple rules gather from felix must be costomed to an array even if there is only one property.          * This is due to the cardinality setting in "@Property".          */
comment|/**          * step 2.1          */
name|obj
operator|=
name|context
operator|.
name|getProperties
argument_list|()
operator|.
name|get
argument_list|(
name|DULCIFIER_RECIPE_RULE_PREFIX
argument_list|)
expr_stmt|;
name|String
index|[]
name|ruleBasePrefix
decl_stmt|;
if|if
condition|(
name|obj
operator|instanceof
name|String
index|[]
condition|)
block|{
name|ruleBasePrefix
operator|=
operator|(
name|String
index|[]
operator|)
name|obj
expr_stmt|;
block|}
else|else
block|{
name|String
index|[]
name|aux
init|=
operator|new
name|String
index|[
literal|1
index|]
decl_stmt|;
name|aux
index|[
literal|0
index|]
operator|=
operator|(
name|String
operator|)
name|obj
expr_stmt|;
name|ruleBasePrefix
operator|=
name|aux
expr_stmt|;
block|}
name|String
name|kReSRuleSyntax
init|=
literal|""
decl_stmt|;
comment|/**          * We add the prefixes in the rules head.          * The syntax used for expressing the rules is the KReSRule syntax.          */
for|for
control|(
name|String
name|auxruleprefix
range|:
name|ruleBasePrefix
control|)
block|{
name|kReSRuleSyntax
operator|+=
name|auxruleprefix
operator|+
literal|" . "
expr_stmt|;
block|}
comment|/**          * step 2.2          */
name|obj
operator|=
name|context
operator|.
name|getProperties
argument_list|()
operator|.
name|get
argument_list|(
name|DULCIFIER_RECIPE_RULE
argument_list|)
expr_stmt|;
name|String
index|[]
name|ruleSyntax
decl_stmt|;
if|if
condition|(
name|obj
operator|instanceof
name|String
index|[]
condition|)
block|{
name|ruleSyntax
operator|=
operator|(
name|String
index|[]
operator|)
name|obj
expr_stmt|;
block|}
else|else
block|{
name|String
index|[]
name|aux
init|=
operator|new
name|String
index|[
literal|1
index|]
decl_stmt|;
name|aux
index|[
literal|0
index|]
operator|=
operator|(
name|String
operator|)
name|obj
expr_stmt|;
name|ruleSyntax
operator|=
name|aux
expr_stmt|;
block|}
for|for
control|(
name|String
name|auxrule
range|:
name|ruleSyntax
control|)
block|{
name|kReSRuleSyntax
operator|+=
name|auxrule
operator|.
name|replaceAll
argument_list|(
literal|"%2C"
argument_list|,
literal|","
argument_list|)
operator|+
literal|" . "
expr_stmt|;
block|}
if|if
condition|(
name|kReSRuleSyntax
operator|.
name|endsWith
argument_list|(
literal|" . "
argument_list|)
condition|)
name|kReSRuleSyntax
operator|=
name|kReSRuleSyntax
operator|.
name|substring
argument_list|(
literal|0
argument_list|,
name|kReSRuleSyntax
operator|.
name|lastIndexOf
argument_list|(
literal|" . "
argument_list|)
operator|+
literal|1
argument_list|)
expr_stmt|;
name|kReSRuleSyntax
operator|=
name|kReSRuleSyntax
operator|.
name|trim
argument_list|()
expr_stmt|;
name|log
operator|.
name|debug
argument_list|(
literal|"The complete rule to be added is: "
operator|+
name|kReSRuleSyntax
argument_list|)
expr_stmt|;
comment|//String kReSRulePerson = "dbpedia =<http://dbpedia.org/ontology/> . google =<http://rdf.data-vocabulary.org#> . foaf =<http://xmlns.com/foaf/0.1/homepage> . typeRule [is(dbpedia:Person, ?x) -> is(google:Person, ?x)] . nameRule [ values(foaf:name, ?x, ?y) -> values(google:name, ?x, ?y) ] . nickRule [ values(foaf:nick, ?x, ?y) -> values(google:nickname, ?x, ?y) ] . photoRule [ has(dbpedia:thumbnail, ?x, ?y) -> has(google:photo, ?x, ?y) ] . professionRule [has(dbpedia:profession, ?x, ?y) -> has(google:title, ?x, ?y)] . occupationRule [has(dbpedia:occupation, ?x, ?y) -> has(google:title, ?x, ?y)] . roleRule [values(dbpedia:role, ?x, ?y) -> values(google:role, ?x, ?y)] . homepageRule [has(foaf:homepage, ?x, ?y) -> has(google:url, ?x, ?y)] . affiliationRule [has(dbpedia:employer, ?x, ?y) -> has(google:affiliation, ?x, ?y)] . knowsRule [has(foaf:knows, ?x, ?y) -> has(google:friend, ?x, ?y)] . addressRule [values(dbpedia:address, ?x, ?y) -> values(google:address, ?x, ?y)]";
comment|/**          * step 3          */
try|try
block|{
name|ruleStore
operator|.
name|addRuleToRecipe
argument_list|(
name|recipeIRI
operator|.
name|toString
argument_list|()
argument_list|,
name|kReSRuleSyntax
argument_list|)
expr_stmt|;
name|log
operator|.
name|debug
argument_list|(
literal|"Added rules to recipe "
operator|+
name|recipeIRI
operator|.
name|toString
argument_list|()
argument_list|)
expr_stmt|;
block|}
catch|catch
parameter_list|(
name|NoSuchRecipeException
name|e
parameter_list|)
block|{
name|log
operator|.
name|error
argument_list|(
literal|"The recipe does not exists: "
argument_list|,
name|e
argument_list|)
expr_stmt|;
block|}
name|log
operator|.
name|info
argument_list|(
literal|"Activated Dulcifier engine"
argument_list|)
expr_stmt|;
block|}
specifier|protected
name|void
name|deactivate
parameter_list|(
name|ComponentContext
name|context
parameter_list|)
block|{
comment|/** Deactivating the dulcifier. The procedure require: 		 * 1) get all the rules from the recipe 		 * 2) remove the recipe. 		 * 3) remove the single rule. 		 * 4) tear down the scope ontologySpace and the scope itself. 		 */
try|try
block|{
comment|/** 			 * step 1: get all the rule 			 */
name|KReSRuleList
name|recipeRuleList
init|=
name|ruleStore
operator|.
name|getRecipe
argument_list|(
name|recipeIRI
argument_list|)
operator|.
name|getkReSRuleList
argument_list|()
decl_stmt|;
comment|/** 			 * step 2: remove the recipe 			 */
if|if
condition|(
name|ruleStore
operator|.
name|removeRecipe
argument_list|(
name|recipeIRI
argument_list|)
condition|)
block|{
name|log
operator|.
name|info
argument_list|(
literal|"The recipe "
operator|+
name|recipeIRI
operator|+
literal|" has been removed correctly"
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|log
operator|.
name|error
argument_list|(
literal|"The recipe "
operator|+
name|recipeIRI
operator|+
literal|" can not be removed"
argument_list|)
expr_stmt|;
block|}
comment|/**     		 * step 3: remove the rules     		 */
for|for
control|(
name|KReSRule
name|rule
range|:
name|recipeRuleList
control|)
block|{
if|if
condition|(
name|ruleStore
operator|.
name|removeRule
argument_list|(
name|rule
argument_list|)
condition|)
block|{
name|log
operator|.
name|info
argument_list|(
literal|"The rule "
operator|+
name|rule
operator|.
name|getRuleName
argument_list|()
operator|+
literal|" has been removed correctly"
argument_list|)
expr_stmt|;
block|}
else|else
block|{
name|log
operator|.
name|error
argument_list|(
literal|"The rule "
operator|+
name|rule
operator|.
name|getRuleName
argument_list|()
operator|+
literal|" can not be removed"
argument_list|)
expr_stmt|;
block|}
block|}
comment|/**     		 * step 4:     		 */
name|scope
operator|.
name|getCoreSpace
argument_list|()
operator|.
name|tearDown
argument_list|()
expr_stmt|;
name|scope
operator|.
name|tearDown
argument_list|()
expr_stmt|;
comment|//          	      //Step x: remove the ontology??????
comment|//                OntologySpace ontologySpace = scope.getCoreSpace();
comment|//                System.err.println(":::::::::::::: SCOPE "+)
comment|//                //Collect all the ontologies
comment|//                Iterator<OWLOntology> ontologies = ontologySpace.getOntologies().iterator();
comment|//
comment|//                //Remove the single ontology
comment|//                ontologySpace.tearDown();
comment|//                while(ontologies.hasNext()){
comment|//                    OWLOntology ontology = ontologies.next();
comment|//                    OntologyInputSource ois = oisForScope(ontology.getOWLOntologyManager().getOntologyDocumentIRI(ontology).toURI().toString());
comment|//                    try{
comment|//                        ontologySpace.removeOntology(ois);
comment|//                    } catch (OntologySpaceModificationException ex) {
comment|//                        log.error("Problem to remove the ontology "+ontology.getOntologyID(),ex);
comment|//                  }
comment|//                }
comment|//                ontologySpace.setUp();
comment|//
comment|//                //Step y: de-registring the scope ???????
comment|//                onManager.getScopeRegistry().deregisterScope(scope);
block|}
catch|catch
parameter_list|(
name|NoSuchRecipeException
name|ex
parameter_list|)
block|{
name|log
operator|.
name|error
argument_list|(
literal|"The recipe "
operator|+
name|recipeIRI
operator|+
literal|" doesn't exist"
argument_list|,
name|ex
argument_list|)
expr_stmt|;
block|}
name|log
operator|.
name|info
argument_list|(
literal|"Deactivated Dulcifier engine"
argument_list|)
expr_stmt|;
block|}
annotation|@
name|Override
specifier|public
name|Map
argument_list|<
name|String
argument_list|,
name|Object
argument_list|>
name|getServiceProperties
parameter_list|()
block|{
return|return
name|Collections
operator|.
name|unmodifiableMap
argument_list|(
name|Collections
operator|.
name|singletonMap
argument_list|(
name|ServiceProperties
operator|.
name|ENHANCEMENT_ENGINE_ORDERING
argument_list|,
operator|(
name|Object
operator|)
name|ServiceProperties
operator|.
name|ORDERING_POST_PROCESSING
argument_list|)
argument_list|)
return|;
block|}
block|}
end_class

end_unit

